{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb9076f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb9076f1",
        "outputId": "777c75cb-be1b-4781-960b-6b41e469796b"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rxsa1-AcTYQ5",
      "metadata": {
        "id": "rxsa1-AcTYQ5"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-z7Pt7XATum7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z7Pt7XATum7",
        "outputId": "6702fa4b-beae-43b1-de04-117196111295"
      },
      "outputs": [],
      "source": [
        "print(\"Insertar el API Key de OpenAI\")\n",
        "openai_key = getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ComCnesKWol-",
      "metadata": {
        "id": "ComCnesKWol-"
      },
      "source": [
        "# Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f67021f0",
      "metadata": {
        "id": "f67021f0"
      },
      "source": [
        "Instanciando el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d38b8865",
      "metadata": {
        "id": "d38b8865"
      },
      "outputs": [],
      "source": [
        "# Importamos la clase ChatOpenAI desde LangChain,\n",
        "# que nos permite conectarnos a modelos de OpenAI como GPT-4, GPT-3.5, etc.\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Creamos una instancia del modelo de lenguaje (LLM).\n",
        "llm = ChatOpenAI(\n",
        "    # Especificamos el modelo a usar. En este caso \"gpt-4o-mini\".\n",
        "    model=\"gpt-4o-mini\",\n",
        "    \n",
        "    # La temperatura controla la \"creatividad\" o \"aleatoriedad\" de las respuestas:\n",
        "    # - Valores bajos (ej. 0.0 o 0.2): respuestas más predecibles, consistentes y deterministas.\n",
        "    # - Valores medios (ej. 0.7): balance entre creatividad y coherencia.\n",
        "    # - Valores altos (ej. 1.0 o más): respuestas más variadas, creativas, pero menos controladas.\n",
        "    temperature=0.7,\n",
        "    \n",
        "    openai_api_key=openai_key\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d501a6ce",
      "metadata": {
        "id": "d501a6ce"
      },
      "source": [
        "## Prueba de mensaje simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e6ecc1e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e6ecc1e",
        "outputId": "07131fee-19f6-43da-b635-3d601555b669"
      },
      "outputs": [],
      "source": [
        "respuesta = llm.invoke(\"Hola\")\n",
        "print(respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acZ4eteCUdut",
      "metadata": {
        "id": "acZ4eteCUdut"
      },
      "source": [
        "## Analisis de la respuesta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PbrT9kPyUfZq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbrT9kPyUfZq",
        "outputId": "58a08b03-aa72-4a81-9c0c-23bd3e9e0e38"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "data = respuesta.__dict__\n",
        "print(json.dumps(data, indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UE5r0I0qVAKN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE5r0I0qVAKN",
        "outputId": "5c2fda2d-24b3-415a-b572-c20553aec601"
      },
      "outputs": [],
      "source": [
        "#dejamos solo el string de la respuesta que nos interesa sin el json\n",
        "print(respuesta.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QlpkroohVmHg",
      "metadata": {
        "id": "QlpkroohVmHg"
      },
      "source": [
        "## Lista de mensajes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EwnpqyEzVnz2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwnpqyEzVnz2",
        "outputId": "19ef46a4-5c05-4f42-e8eb-aa71105b7713"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"Eres un asistente que traduce del español a binario. Traduce las oraciones del usuario\",\n",
        "    ),\n",
        "    (\"human\", \"Me gusta la programacion\"),\n",
        "]\n",
        "ai_msg = llm.invoke(messages)\n",
        "ai_msg\n",
        "data = ai_msg.__dict__\n",
        "print(json.dumps(data, indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kxPn7uCfV0PX",
      "metadata": {
        "id": "kxPn7uCfV0PX"
      },
      "source": [
        "## Chaining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o1_LW0byV1p_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1_LW0byV1p_",
        "outputId": "5aea38cb-3a4e-4b74-d849-25f8eab5329d"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Eres un asistente que traduce del {input_language} al {output_language}. Traduce las oraciones del usuario\",\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "respuesta = chain.invoke(\n",
        "    {\n",
        "        \"input_language\": \"español\",\n",
        "        \"output_language\": \"binario\",\n",
        "        \"input\": \"Me gusta la programacion\",\n",
        "    }\n",
        ")\n",
        "respuesta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7Z9QIibvV_gn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z9QIibvV_gn",
        "outputId": "4139fed2-024b-4307-b41e-05b028f6fc85"
      },
      "outputs": [],
      "source": [
        "print(respuesta.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KHFbOi1oWrSY",
      "metadata": {
        "id": "KHFbOi1oWrSY"
      },
      "source": [
        "# Messages\n",
        "\n",
        "Los mensajes son la unidad de comunicación en los [modelos de chat](/docs/concepts/chat_models). Se utilizan para representar la entrada y salida de un modelo de chat, así como cualquier contexto adicional o metadatos que puedan estar asociados a una conversación.\n",
        "\n",
        "Cada mensaje tiene un **rol** (por ejemplo, \"user\", \"assistant\") y un **contenido** (por ejemplo, texto o datos multimodales), junto con metadatos adicionales que varían según el proveedor del modelo de chat.\n",
        "\n",
        "LangChain proporciona un formato de mensaje unificado que puede usarse en diferentes modelos de chat, lo que permite a los usuarios trabajar con distintos modelos sin preocuparse por los detalles específicos del formato de mensajes de cada proveedor.\n",
        "\n",
        "## ¿Qué contiene un mensaje?\n",
        "\n",
        "Un mensaje normalmente consta de la siguiente información:\n",
        "\n",
        "* **Rol**: El rol del mensaje (por ejemplo, \"user\", \"assistant\").\n",
        "* **Contenido**: El contenido del mensaje (por ejemplo, texto o datos multimodales).\n",
        "* Metadatos adicionales: id, nombre, [uso de tokens](/docs/concepts/tokens) y otros metadatos específicos del modelo.\n",
        "\n",
        "### Rol\n",
        "\n",
        "Los roles se utilizan para distinguir entre diferentes tipos de mensajes en una conversación y ayudar al modelo de chat a entender cómo responder a una secuencia dada de mensajes.\n",
        "\n",
        "| **Rol**               | **Descripción**                                                                                                                                                                                                                                              |\n",
        "| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| **system**            | Se utiliza para indicar al modelo de chat cómo comportarse y proporcionar contexto adicional. No es compatible con todos los proveedores de modelos.                                                                                                         |\n",
        "| **user**              | Representa la entrada de un usuario que interactúa con el modelo, generalmente en forma de texto u otra entrada interactiva.                                                                                                                                 |\n",
        "| **assistant**         | Representa una respuesta del modelo, que puede incluir texto o una solicitud para invocar herramientas.                                                                                                                                                      |\n",
        "| **tool**              | Mensaje utilizado para devolver al modelo los resultados de la invocación de una herramienta, después de haber obtenido datos o realizado un procesamiento externo. Usado con modelos que admiten [invocación de herramientas](/docs/concepts/tool_calling). |\n",
        "| **function** (legado) | Rol heredado, correspondiente a la API antigua de llamadas a funciones de OpenAI. Debería usarse el rol **tool** en su lugar.                                                                                                                                |\n",
        "\n",
        "### Contenido\n",
        "\n",
        "El contenido de un mensaje puede ser texto o una lista de diccionarios que representan [datos multimodales](/docs/concepts/multimodality) (por ejemplo, imágenes, audio, video). El formato exacto del contenido puede variar entre diferentes proveedores de modelos de chat.\n",
        "\n",
        "Actualmente, la mayoría de modelos de chat admiten texto como tipo principal de contenido, aunque algunos también admiten datos multimodales. Sin embargo, el soporte para datos multimodales aún es limitado.\n",
        "\n",
        "## Estructura de la conversación\n",
        "\n",
        "La secuencia de mensajes en un modelo de chat debe seguir una estructura específica para garantizar que el modelo genere una respuesta válida.\n",
        "\n",
        "Por ejemplo, una estructura típica de conversación podría ser:\n",
        "\n",
        "1. **Mensaje del usuario**: \"Hola, ¿cómo estás?\"\n",
        "2. **Mensaje del asistente**: \"Estoy bien, gracias por preguntar.\"\n",
        "3. **Mensaje del usuario**: \"¿Puedes contarme un chiste?\"\n",
        "4. **Mensaje del asistente**: \"¡Claro! ¿Por qué el espantapájaros ganó un premio? Porque era sobresaliente en su campo.\"\n",
        "\n",
        "## Mensajes de LangChain\n",
        "\n",
        "LangChain proporciona un formato unificado de mensajes que puede usarse con todos los modelos de chat, lo que permite a los usuarios trabajar con distintos modelos sin preocuparse por los detalles del formato de mensajes de cada uno.\n",
        "\n",
        "Los mensajes de LangChain son objetos de Python que heredan de [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html).\n",
        "\n",
        "Los cinco tipos principales de mensajes son:\n",
        "\n",
        "* [SystemMessage](#systemmessage): corresponde al rol **system**\n",
        "* [HumanMessage](#humanmessage): corresponde al rol **user**\n",
        "* [AIMessage](#aimessage): corresponde al rol **assistant**\n",
        "* [AIMessageChunk](#aimessagechunk): también corresponde al rol **assistant**, pero se usa para respuestas por [streaming](/docs/concepts/streaming)\n",
        "* [ToolMessage](#toolmessage): corresponde al rol **tool**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OlN0Eh9iXdc-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlN0Eh9iXdc-",
        "outputId": "076961ab-1cae-4506-b376-ed0d78f9e4b5"
      },
      "outputs": [],
      "source": [
        "# Importamos los tipos de mensajes que se usan en LangChain para simular un historial de conversación\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# Definimos una lista de mensajes que representan el historial de la conversación\n",
        "mensajes = [\n",
        "    # Mensaje del sistema: define el rol y comportamiento del asistente\n",
        "    SystemMessage(content='Eres un asistente que resuelve operaciones matemáticas y cuya respuesta es directa.'),\n",
        "    # Primer turno de conversación: el humano pregunta\n",
        "    HumanMessage(content='¿Cuánto es 1 + 1?'),\n",
        "    # El asistente responde\n",
        "    AIMessage(content='2'),\n",
        "    # Segundo turno: otra pregunta del humano\n",
        "    HumanMessage(content='¿Cuánto es 10 * 5?'),\n",
        "    # Respuesta del asistente\n",
        "    AIMessage(content='50'),\n",
        "    # Tercer turno: otra pregunta\n",
        "    HumanMessage(content='¿Cuánto es 10 + 3?'),\n",
        "]\n",
        "\n",
        "# Ejemplo de cómo invocar al modelo directamente con la lista de mensajes (comentado)\n",
        "# chat.invoke(mensajes)\n",
        "# Aquí se hace streaming de la respuesta del modelo usando los mensajes anteriores como contexto.\n",
        "# El modelo generará la respuesta en fragmentos (\"chunks\").\n",
        "for chunk in llm2 .stream(mensajes):\n",
        "    # Imprime cada pedazo de la respuesta generada en tiempo real\n",
        "    print(chunk.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hPjB9peKXrXt",
      "metadata": {
        "id": "hPjB9peKXrXt"
      },
      "source": [
        "Esto es similar a la estructura de mensajes de la API de OpenAI, pero con una sintaxis diferente:\n",
        "\n",
        "```python\n",
        "mensajes = [\n",
        "    {'role': 'user', 'content': 'Cuánto es 1 + 1'},\n",
        "    {'role': 'assistant', 'content': '2'},\n",
        "    {'role': 'user', 'content': 'Cuánto es 10 * 5'},\n",
        "    {'role': 'assistant', 'content': '50'},\n",
        "    {'role': 'user', 'content': 'Cuánto es 10 + 3'},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XnM4mjiAXqzU",
      "metadata": {
        "id": "XnM4mjiAXqzU"
      },
      "outputs": [],
      "source": [
        "## Probando otros modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RBpNa5XJX5-6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBpNa5XJX5-6",
        "outputId": "f663d73a-3a0f-408e-84a4-fe14dba0eaaa"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-nvidia-ai-endpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WcFw_6FrYU9_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcFw_6FrYU9_",
        "outputId": "eec92565-91ec-4985-d187-874e527e0104"
      },
      "outputs": [],
      "source": [
        "\n",
        "#obtener api key de https://build.nvidia.com/\n",
        "\n",
        "from getpass import getpass\n",
        "print(\"Insertar el API Key de Nvidia\")\n",
        "nvidia_key = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pPjHAxRLX5ii",
      "metadata": {
        "id": "pPjHAxRLX5ii"
      },
      "outputs": [],
      "source": [
        "## Core LC Chat Interface\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "llm2 = ChatNVIDIA(model=\"meta/llama-4-maverick-17b-128e-instruct\", api_key=nvidia_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-V4kLKWMYs-f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V4kLKWMYs-f",
        "outputId": "623610ab-98af-4e24-d944-3895005ed53a"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "mensajes = [\n",
        "    SystemMessage(content='Eres un asistente que resuelve operaciones matemáticas y cuya respuesta es directa.'),\n",
        "    HumanMessage(content='¿Cuánto es 1 + 1?'),\n",
        "    AIMessage(content='2'),\n",
        "    HumanMessage(content='¿Cuánto es 10 * 5?'),\n",
        "    AIMessage(content='50'),\n",
        "    HumanMessage(content='¿Cuánto es 10 + 3?'),\n",
        "]\n",
        "\n",
        "# chat.invoke(mensajes)\n",
        "\n",
        "for chunk in llm2.stream(mensajes):\n",
        "    print(chunk.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sqywSdbjYxXC",
      "metadata": {
        "id": "sqywSdbjYxXC"
      },
      "source": [
        "# Streams\n",
        "\n",
        "La generación de respuestas completas desde LLM suele conllevar un retraso de varios segundos, que se hace más evidente en aplicaciones complejas con múltiples llamadas a modelos. Afortunadamente, los LLM generan respuestas de forma iterativa, lo que permite mostrar resultados intermedios a medida que se generan. Al transmitir estos resultados intermedios, LangChain facilita una experiencia de usuario más fluida en aplicaciones basadas en LLM y ofrece compatibilidad integrada con la transmisión en su diseño."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sAWdZxG5YytG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAWdZxG5YytG",
        "outputId": "22a05dc8-6567-4cab-9a1c-ed73df6f499d"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "mensajes = [\n",
        "    SystemMessage(content='Eres un asistente que cuenta chistes.'),\n",
        "    HumanMessage(content='¿Cuánto es 1 + 1?')\n",
        "]\n",
        "\n",
        "for tramo in llm.stream(mensajes):\n",
        "    print(tramo.content, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4kPrvD5qZjGZ",
      "metadata": {
        "id": "4kPrvD5qZjGZ"
      },
      "source": [
        "# Caching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2Wma8SjnZ3ve",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wma8SjnZ3ve",
        "outputId": "2bc75afb-6655-4f63-d532-c26de4efe187"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5uwg7fRLZmb5",
      "metadata": {
        "id": "5uwg7fRLZmb5"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "mensajes = [\n",
        "    SystemMessage(content='Eres un narrador de historias cortas'),\n",
        "    HumanMessage(content='Sobre programacion')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "flC0ieJvZoej",
      "metadata": {
        "id": "flC0ieJvZoej"
      },
      "outputs": [],
      "source": [
        "from langchain.cache import InMemoryCache\n",
        "from langchain.globals import set_llm_cache\n",
        "\n",
        "set_llm_cache(InMemoryCache())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PO-FxVGraF7W",
      "metadata": {
        "id": "PO-FxVGraF7W"
      },
      "source": [
        "Primera vez"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t4SHdMlbZ-pM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4SHdMlbZ-pM",
        "outputId": "b42d5a98-3ae5-4a73-f3b0-646f0ca2e428"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "llm2.invoke(mensajes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_6304HVIaIHq",
      "metadata": {
        "id": "_6304HVIaIHq"
      },
      "source": [
        "Segunda vez"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QFQgADpIaFAg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFQgADpIaFAg",
        "outputId": "bfede709-d7b0-4fd2-a139-070ddb45e138"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "llm2.invoke(mensajes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dqf8Jptuab8O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqf8Jptuab8O",
        "outputId": "a2d3fb16-fe6e-4fa8-990d-545de04930d3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "llm2.invoke(mensajes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pgEMEB7FagRi",
      "metadata": {
        "id": "pgEMEB7FagRi"
      },
      "source": [
        "# Output Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uq3c_bcVahi2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq3c_bcVahi2",
        "outputId": "03e6728b-feeb-4480-8e27-72286e9489e5"
      },
      "outputs": [],
      "source": [
        "review_cliente = \"\"\"Estos audífonos inalámbricos superaron completamente mis expectativas. Tienen\n",
        "cancelación de ruido activa, modo transparencia y duración de batería de hasta 30 horas. Los pedí\n",
        "el lunes y llegaron el miércoles, perfecto para mi viaje de trabajo del viernes. El sonido es\n",
        "cristalino y los graves son impresionantes. Mi hermano me había recomendado esta marca después de\n",
        "probárselos. Aunque costaron $180 dólares, que es más de lo que normalmente gastaría en audífonos,\n",
        "definitivamente valen cada centavo por la calidad de construcción y las características premium que\n",
        "ofrecen. Los he usado durante largas sesiones de trabajo y son muy cómodos.\"\"\"\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "review_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "Para el siguiente texto, extrae la siguiente información:\n",
        "\n",
        "recomendacion_externa: ¿El producto fue recomendado por alguien más? True si es verdadero y False si es falso\n",
        "o no se encuentra la información.\n",
        "\n",
        "dias_entrega: ¿Cuántos días tardó en llegar la entrega? Si no se encuentra la respuesta, devuelve -1.\n",
        "\n",
        "precio_mencionado: Extrae cualquier precio específico mencionado en el review. Si no hay precio específico,\n",
        "devuelve null.\n",
        "\n",
        "caracteristicas_destacadas: Extrae las características técnicas o funcionales mencionadas del producto.\n",
        "Devuelve el resultado como una lista en Python.\n",
        "\n",
        "justificacion_precio: Extrae cualquier frase que justifique el costo o valor del producto. Devuelve el\n",
        "resultado como una lista en Python.\n",
        "\n",
        "Texto: {review}\n",
        "\n",
        "Retorna la respuesta en formato JSON\n",
        "\"\"\")\n",
        "\n",
        "print(review_template.format_messages(review=review_cliente))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nphd1HHmbLrr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nphd1HHmbLrr",
        "outputId": "ddbeddda-be4a-467f-deeb-cb93e2702c48"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "respuesta = llm.invoke(review_template.format_messages(review=review_cliente))\n",
        "print(respuesta.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
